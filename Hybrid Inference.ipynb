{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bf7ff3e-af0a-4bdb-8925-abb7a3e5f7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('words_250000_train.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "rng = np.random.default_rng(89)\n",
    "\n",
    "data = rng.permutation(words)\n",
    "# data=data[:500]\n",
    "split_index= int(0.95 * len(data))\n",
    "train_words = data[:split_index]\n",
    "val_words = data[-1000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8631a221-e111-4980-b5fa-c97a6f312b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 18:28:21.494419: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-16 18:28:21.494481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-16 18:28:21.495578: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-16 18:28:21.501213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-16 18:28:22.354467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Bidirectional, LSTM, Dropout, TimeDistributed, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "473c6438-ff93-44da-beb3-4421b5583cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class LSTMWordPredictor:\n",
    "#     def __init__(self, weights_path=\"lstm_model6.h5\", max_word_length=20):\n",
    "#         self.chars = list(\"abcdefghijklmnopqrstuvwxyz0\")\n",
    "#         self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
    "#         self.int_to_char = {i: c for i, c in enumerate(self.chars)}\n",
    "#         self.vocab_size = len(self.chars)\n",
    "#         self.max_word_length = max_word_length\n",
    "#         self.model = self.build_model()\n",
    "#         self.model.load_weights(weights_path)\n",
    "\n",
    "#     def build_model(self):\n",
    "#         model = Sequential()\n",
    "#         model.add(Embedding(input_dim=self.vocab_size, output_dim=64, trainable=True))\n",
    "#         model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "#         model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001))))\n",
    "#         model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001))))\n",
    "#         model.add(Dropout(0.4))\n",
    "#         model.add(TimeDistributed(Dense(self.vocab_size, activation='softmax')))\n",
    "#         model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#         return model\n",
    "\n",
    "#     def predict(self, word_with_missing, guessed_letters):\n",
    "#         word_encoded = [self.char_to_int.get(char, self.char_to_int['0']) for char in word_with_missing]\n",
    "#         word_padded = pad_sequences([word_encoded], maxlen=self.max_word_length, padding='post')\n",
    "#         prediction = self.model.predict(word_padded, verbose=0)[0]\n",
    "#         return prediction\n",
    "#         best_char = None\n",
    "#         best_prob = -1\n",
    "\n",
    "#         for i, char in enumerate(word_with_missing):\n",
    "#             if char == '0':\n",
    "#                 probabilities = prediction[i]\n",
    "#                 return probabilities\n",
    "#                 for idx in np.argsort(-probabilities):\n",
    "#                     predicted_char = self.int_to_char[idx]\n",
    "#                     if predicted_char != '0' and predicted_char not in guessed_letters:\n",
    "#                         prob = probabilities[idx]\n",
    "#                         if prob > best_prob:\n",
    "#                             best_prob = prob\n",
    "#                             best_char = predicted_char\n",
    "#                         break\n",
    "#         return best_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab00f51-9c19-47fa-857f-ab7df1fd265d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "class HangmanXGBoostPredictor:\n",
    "    def __init__(self, model_path=\"xgboost_hangman_models3.pkl\", pos_features=65):\n",
    "        self.alphabet = list(string.ascii_lowercase)\n",
    "        self.num_classes = len(self.alphabet)\n",
    "        self.models = []\n",
    "        self.pos_features = pos_features\n",
    "        self._load_models(model_path)\n",
    "\n",
    "    def _load_models(self, model_path):\n",
    "        with open(model_path, 'rb') as f:\n",
    "            self.models = pickle.load(f)\n",
    "\n",
    "    def _encode_word_state(self, word: str, guessed_letters: set) -> np.ndarray:\n",
    "        feat = np.full(self.pos_features, -1, dtype=np.int8)\n",
    "        length = len(word)\n",
    "        offset = self.pos_features - length\n",
    "\n",
    "        for i, ch in enumerate(word):\n",
    "            code = 0 if ch == '*' else ord(ch) - ord('a') + 1\n",
    "            feat[i] = code\n",
    "            feat[offset + i] = code\n",
    "\n",
    "        return feat.reshape(1, -1)\n",
    "\n",
    "    def predict_letter(self, word: str, guessed_letters: set) -> str:\n",
    "        features = self._encode_word_state(word, guessed_letters)\n",
    "        scores = []\n",
    "        # print(self.models)\n",
    "        for idx, model in enumerate(self.models.items()):\n",
    "            # print(idx,model)\n",
    "            model=model[1]\n",
    "            letter = self.alphabet[idx]\n",
    "            if letter in guessed_letters:\n",
    "                scores.append(-np.inf)\n",
    "                continue\n",
    "            prob = model.predict_proba(features)[0][1]  # probability of presence\n",
    "            scores.append(prob)\n",
    "\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82f6bea1-ba53-4259-a9cb-75ce1ee6e58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CanineTokenizer, CanineConfig, CanineForSequenceClassification\n",
    "from transformers import CanineConfig, CanineTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# # Constants for separating and masking in the CANINE input sequence\n",
    "# CANINE_SEP_TOKEN   = \" [SEP] \"\n",
    "# CANINE_MASK_TOKEN  = \"[MASK]\"\n",
    "\n",
    "class CanineHangmanPlayer:\n",
    "    def __init__(self, pretrained_model_path: str = \"google/canine-s\", device: torch.device = None,xgb=None):\n",
    "        # Load CANINE tokenizer & config\n",
    "        self.tokenizer = CanineTokenizer.from_pretrained('google/canine-s')\n",
    "        self.config    = CanineConfig.from_pretrained(pretrained_model_path)\n",
    "        self.config.num_labels = 26\n",
    "        self.xgb=xgb\n",
    "        self.nice=1\n",
    "        self.trigger=1\n",
    "        self.all_letters = [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
    "        self.state=False\n",
    "        self.guesses={}\n",
    "\n",
    "        \n",
    "        # Set up device\n",
    "        self.device = \"cuda\"#device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load a sequence-classification head on top of CANINE\n",
    "        self.model =AutoModelForSequenceClassification.from_pretrained(\n",
    "            pretrained_model_path,\n",
    "            config=self.config).to(self.device)\n",
    "        \n",
    "        # Tokens for building the game state string\n",
    "        self.CANINE_MASK_TOKEN = self.tokenizer.mask_token\n",
    "        self.CANINE_SEP_TOKEN = self.tokenizer.sep_token\n",
    "        \n",
    "        # Toggle for self-play finetuning\n",
    "        self.training = False\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def simulate_hangman_transformers(self, word: str, max_wrong_guesses: int = 6, verbose: int = 1):\n",
    "        \"\"\"\n",
    "        Play hangman against the CANINE model.\n",
    "        If self.training is True, returns (model_logits_seq, true_dist_seq, success_flag).\n",
    "        Otherwise returns just susccess_flag.\n",
    "        \"\"\"\n",
    "        # Build index map of letters→positions\n",
    "        word_idxs = {}\n",
    "        for i, c in enumerate(word):\n",
    "            word_idxs.setdefault(c, []).append(i)\n",
    "\n",
    "        all_letters = [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
    "        guesses      = {}\n",
    "        encoded_word = \"*\" * len(word)\n",
    "        num_wrong= 0\n",
    "\n",
    "        self.eval()\n",
    "        if self.training:\n",
    "            outputs_model = []\n",
    "            outputs_true  = []\n",
    "\n",
    "        # if verbose:\n",
    "        #     print(f\"[WORD]: {word}\")\n",
    "\n",
    "\n",
    "        while encoded_word != word and num_wrong < max_wrong_guesses:\n",
    "            missing_count = encoded_word.count(\"*\")\n",
    "\n",
    "            # if missing_count <= 0:\n",
    "            #     masked = encoded_word.replace(\"*\", \"0\")\n",
    "            #     guess = self.lstm_model.predict(masked, guessed_letters=set(guesses.keys()))\n",
    "            #     print(guess,end=\" \")\n",
    "            # else:\n",
    "            \n",
    "            check = self.xgb.predict_letter(''.join(encoded_word), guesses)\n",
    "            # encoded_word = encoded_word.replace(\"*\", \"0\")\n",
    "            # check = self.xgb.predict(''.join(encoded_word), guesses)\n",
    "            # encoded_word = encoded_word.replace(\"0\", \"*\")\n",
    "\n",
    "            \n",
    "            state = ''.join(guesses.keys()) + self.CANINE_SEP_TOKEN + encoded_word.replace('*', self.CANINE_MASK_TOKEN)\n",
    "            enc = self.tokenizer(state, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**enc).logits\n",
    "\n",
    "            arr = logits.cpu().numpy()[0]\n",
    "            arr1=np.argsort(arr)[::-1].tolist()\n",
    "            total = sum(arr)\n",
    "            guess_idx = np.argmax(arr)\n",
    "            arr = [x / total for x in arr]\n",
    "            arr1=np.argsort(check)[::-1].tolist()\n",
    "            rank1=np.argsort(check)[::-1].tolist()\n",
    "            pos1=rank1.index(guess_idx)\n",
    "            pos2=all_letters[guess_idx]\n",
    "            # guess = all_letters[guess_idx]\n",
    "            org=arr[guess_idx]\n",
    "            verify=check[guess_idx]\n",
    "            # trigger=0\n",
    "            trigger=False\n",
    "            if org<0.005 and pos1>=7 and missing_count<=4:\n",
    "                # count+=1\n",
    "                # change=1\n",
    "                # print(\"Trigger\")\n",
    "                trigger=True\n",
    "                # self.trigger+=1\n",
    "                guess_idx = rank1[0]\n",
    "            guess = all_letters[guess_idx]\n",
    "\n",
    "\n",
    "            if not trigger:\n",
    "                while guess in guesses:\n",
    "                    arr[guess_idx] = -np.inf \n",
    "                    guess_idx = np.argmax(arr)\n",
    "                    guess = all_letters[guess_idx]\n",
    "            \n",
    "            if pos2!=guess and pos2 not in word_idxs and guess not in word_idxs:\n",
    "                self.trigger-=1\n",
    "            # Apply the guess\n",
    "            if guess in word_idxs:\n",
    "                self.nice+=1 if trigger else 0\n",
    "                # self.trigger+=1\n",
    "                for pos in word_idxs[guess]:\n",
    "                    encoded_word = encoded_word[:pos] + guess + encoded_word[pos+1:]\n",
    "            else:\n",
    "                self.trigger+=1 if trigger else 0\n",
    "                num_wrong += 1\n",
    "            trigger=False\n",
    "\n",
    "            guesses[guess] = True\n",
    "            if verbose == 1:\n",
    "                print(f\"  Guess: {guess.upper():<2} → {encoded_word}  (Wrong: {num_wrong}) {verify:.3f} {org:.3f} {pos1}, 'Org', {pos2} {all_letters[rank1[0]]} \")\n",
    "\n",
    "        success = (encoded_word == word)\n",
    "        if verbose == 3 or verbose==1:\n",
    "            print(f\"Result: {'✅ CORRECT' if success else '❌ FAILED'} | Final: {encoded_word} {word}  \\n\")\n",
    "        # print(nice/count*100)\n",
    "        if self.training:\n",
    "            return torch.vstack(outputs_model), torch.vstack(outputs_true), success\n",
    "        return success\n",
    "    \n",
    "    def test_accuracy(self, words, verbose=1):\n",
    "        # global nice,trigger\n",
    "        # nice,trigger=1,1\n",
    "        n=len(words)\n",
    "        count=1\n",
    "        correct = 0\n",
    "        for w in words:\n",
    "            print(f\"{(correct / count) * 100:.2f} {count} / {n} {self.nice} {self.trigger} \", end=\"\\r\" )\n",
    "            correct += self.simulate_hangman_transformers(w, verbose=verbose)\n",
    "            count+=1\n",
    "        return correct / len(words)\n",
    "    def predict(self,word):\n",
    "                \n",
    "            encoded_word=word.replace(\" \",\"\").replace(\"_\",\"*\")\n",
    "            if self.state==True and set(list(word))==set([\"*\"]):\n",
    "                self.state=False\n",
    "                self.guesses={}\n",
    "                \n",
    "\n",
    "            check = self.xgb.predict_letter(''.join(encoded_word), self.guesses)\n",
    "            state = ''.join(guesses.keys()) + self.CANINE_SEP_TOKEN + encoded_word.replace('*', self.CANINE_MASK_TOKEN)\n",
    "            enc = self.tokenizer(state, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**enc).logits\n",
    "\n",
    "            arr = logits.cpu().numpy()[0]\n",
    "            arr1=np.argsort(arr)[::-1].tolist()\n",
    "            \n",
    "            total = sum(arr)\n",
    "            guess_idx = np.argmax(arr)\n",
    "            arr = [x / total for x in arr]\n",
    "            \n",
    "            arr1=np.argsort(check)[::-1].tolist()\n",
    "            rank1=np.argsort(check)[::-1].tolist()\n",
    "            pos1=rank1.index(guess_idx)\n",
    "            # guess = all_letters[guess_idx]\n",
    "            org=arr[guess_idx]\n",
    "            verify=check[guess_idx]\n",
    "\n",
    "            if org<0.009 and pos1>=5 and missing_count<=4:\n",
    "                # print(\"Trigger\")\n",
    "                trigger=True\n",
    "                guess_idx = rank1[0]\n",
    "            guess = all_letters[guess_idx]\n",
    "            guesses[guess]=True\n",
    "            return guess\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf32c872-e614-4ae0-9120-e9e89882b464",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HangmanXGBoostPredictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xgb \u001b[38;5;241m=\u001b[39m \u001b[43mHangmanXGBoostPredictor\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgboost_hangman_models2.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m=\u001b[39mCanineHangmanPlayer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,xgb\u001b[38;5;241m=\u001b[39mxgb)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtraining\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HangmanXGBoostPredictor' is not defined"
     ]
    }
   ],
   "source": [
    "xgb = HangmanXGBoostPredictor(\"xgboost_hangman_models2.pkl\")\n",
    "model=CanineHangmanPlayer(\"10epoch\",xgb=xgb)\n",
    "model.training=False\n",
    "print(model.test_accuracy(val_words,verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04253945-0858-4f3e-b8ce-b87eb1d0aa68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb = LSTMWordPredictor(weights_path=\"lstm_model6.h5\")\n",
    "model=CanineHangmanPlayer(\"10epoch\",xgb=xgb)\n",
    "model.training=False\n",
    "print(model.test_accuracy(val_words,verbose=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5cc11-e541-47b4-a9f9-5f668242f0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15b07b-12e5-42c8-81eb-8c0b4c79146e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
