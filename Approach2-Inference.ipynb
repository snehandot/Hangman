{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed50eb9a-cfe0-4830-85aa-40248af2908a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('words_250000_train.txt', 'r') as f:\n",
    "    words = f.read().splitlines()\n",
    "rng = np.random.default_rng(46)\n",
    "data = rng.permutation(words)\n",
    "# data=data[:500]\n",
    "split_index= int(0.95 * len(data))\n",
    "train_words = data[:split_index]\n",
    "val_words = data[-1000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9c7d1f36-2eb9-4884-84e2-8f86b01211f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Bidirectional, LSTM, Dropout, TimeDistributed, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class LSTMWordPredictor:\n",
    "    def __init__(self, weights_path=\"lstm_model4.h5\", max_word_length=20):\n",
    "        self.chars = list(\"abcdefghijklmnopqrstuvwxyz0\")\n",
    "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.int_to_char = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.max_word_length = max_word_length\n",
    "        self.model = self.build_model()\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=self.vocab_size, output_dim=64, trainable=True))\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001))))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(self.vocab_size, activation='softmax')))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "#     def predict(self, word_with_missing, guessed_letters):\n",
    "#         word_with_missing = word_with_missing.replace(' ', '')  # Replace '_' with '0' for consistency\n",
    "#         word_with_missing = word_with_missing.replace('_', '0')  # Replace '_' with '0' for consistency\n",
    "#         word_encoded = [self.char_to_int.get(char, self.char_to_int['0']) for char in word_with_missing]\n",
    "#         word_padded = pad_sequences([word_encoded], maxlen=self.max_word_length, padding='post')\n",
    "#         prediction = self.model.predict(word_padded, verbose=0)[0]\n",
    "\n",
    "#         for i, char in enumerate(word_with_missing):\n",
    "#             if char == '0':\n",
    "#                 probabilities = prediction[i]\n",
    "#                 sorted_indices = np.argsort(-probabilities)\n",
    "#                 pred=[self.int_to_char[idx] for idx in sorted_indices]\n",
    "#                 return pred\n",
    "#                 for idx in sorted_indices:\n",
    "#                     predicted_char = self.int_to_char[idx]\n",
    "#                     if predicted_char != '0' and predicted_char not in guessed_letters:\n",
    "#                         return predicted_char\n",
    "                \n",
    "#         return None\n",
    "    def predict(self, word_with_missing, guessed_letters):\n",
    "        # word_with_missing = word_with_missing.replace(' ', '').replace('_', '0')\n",
    "        word_encoded = [self.char_to_int.get(char, self.char_to_int['0']) for char in word_with_missing]\n",
    "        word_padded = pad_sequences([word_encoded], maxlen=self.max_word_length, padding='post')\n",
    "        prediction = self.model.predict(word_padded, verbose=0)[0]\n",
    "\n",
    "        best_char = None\n",
    "        best_prob = -1\n",
    "\n",
    "        for i, char in enumerate(word_with_missing):\n",
    "            if char == '0':\n",
    "                probabilities = prediction[i]\n",
    "                for idx in np.argsort(-probabilities):\n",
    "                    predicted_char = self.int_to_char[idx]\n",
    "                    if predicted_char != '0' and predicted_char not in guessed_letters:\n",
    "                        prob = probabilities[idx]\n",
    "                        if prob > best_prob:\n",
    "                            best_prob = prob\n",
    "                            best_char = predicted_char\n",
    "                        break  # Only consider top valid char per position\n",
    "\n",
    "        return best_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26918f2f-274a-41f9-b040-285680c153a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\n"
     ]
    }
   ],
   "source": [
    "# words = [\"hello\", \"world\", \"another\", \"example\"]  # your full word list\n",
    "predictor = LSTMWordPredictor(weights_path=\"lstm_model3.h5\")\n",
    "pred = predictor.predict(\"an00r\", guessed_letters={'o','i','s','t','d','a','l','u'})\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3434fafb-ec7f-4372-9984-2406ffe2c92d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 03:04:06.420985: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-15 03:04:06.421047: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-15 03:04:06.422192: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-15 03:04:06.428384: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-15 03:04:07.248396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import ByT5Tokenizer, T5Config, T5ForSequenceClassification\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, Bidirectional, LSTM, Dropout, TimeDistributed, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "BYT5_MASK_TOKEN = \"<extra_id_0>\"\n",
    "BYT5_SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "class LSTMWordPredictor:\n",
    "    def __init__(self, weights_path=\"lstm_model6.h5\", max_word_length=20):\n",
    "        self.chars = list(\"abcdefghijklmnopqrstuvwxyz0\")\n",
    "        self.char_to_int = {c: i for i, c in enumerate(self.chars)}\n",
    "        self.int_to_char = {i: c for i, c in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.max_word_length = max_word_length\n",
    "        self.model = self.build_model()\n",
    "        self.model.load_weights(weights_path)\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=self.vocab_size, output_dim=64, trainable=True))\n",
    "        model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001))))\n",
    "        model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(0.001))))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(TimeDistributed(Dense(self.vocab_size, activation='softmax')))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def predict(self, word_with_missing, guessed_letters):\n",
    "        word_encoded = [self.char_to_int.get(char, self.char_to_int['0']) for char in word_with_missing]\n",
    "        word_padded = pad_sequences([word_encoded], maxlen=self.max_word_length, padding='post')\n",
    "        prediction = self.model.predict(word_padded, verbose=0)[0]\n",
    "\n",
    "        best_char = None\n",
    "        best_prob = -1\n",
    "\n",
    "        for i, char in enumerate(word_with_missing):\n",
    "            if char == '0':\n",
    "                probabilities = prediction[i]\n",
    "                for idx in np.argsort(-probabilities):\n",
    "                    predicted_char = self.int_to_char[idx]\n",
    "                    if predicted_char != '0' and predicted_char not in guessed_letters:\n",
    "                        prob = probabilities[idx]\n",
    "                        if prob > best_prob:\n",
    "                            best_prob = prob\n",
    "                            best_char = predicted_char\n",
    "                        break\n",
    "        return best_char\n",
    "\n",
    "class ByT5HangmanPlayer:\n",
    "    def __init__(self, saved_model_path=None,lstm=None):\n",
    "        self.tokenizer = ByT5Tokenizer.from_pretrained(\"google/byt5-small\")\n",
    "        self.config = T5Config.from_pretrained(\"google/byt5-small\")\n",
    "        self.config.num_labels = 26\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = T5ForSequenceClassification.from_pretrained(\"byt5-checkpoint-epoch7b\", config=self.config).to(self.device)\n",
    "        self.lstm_model = lstm\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def simulate_hangman_transformers(self, word, max_wrong_guesses=6, verbose=1):\n",
    "        word_idxs = {}\n",
    "        all_letters = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "        for i, c in enumerate(word):\n",
    "            word_idxs.setdefault(c, []).append(i)\n",
    "\n",
    "        guesses = {}\n",
    "        encoded_word = \"*\" * len(word)\n",
    "        num_wrong = 0\n",
    "        self.eval()\n",
    "\n",
    "        # if verbose:\n",
    "        #     print(f\"[WORD]: {word}\")\n",
    "\n",
    "        while encoded_word != word and num_wrong < max_wrong_guesses:\n",
    "            missing_count = encoded_word.count(\"*\")\n",
    "\n",
    "            if missing_count <= 0:\n",
    "                masked = encoded_word.replace(\"*\", \"0\")\n",
    "                guess = self.lstm_model.predict(masked, guessed_letters=set(guesses.keys()))\n",
    "                print(guess,end=\" \")\n",
    "            else:\n",
    "                state = ''.join(guesses.keys()) + BYT5_SEP_TOKEN + encoded_word.replace('*', BYT5_MASK_TOKEN)\n",
    "                enc = self.tokenizer(state, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(**enc).logits\n",
    "\n",
    "                arr = logits.cpu().numpy()[0]\n",
    "                guess_idx = np.argmax(arr)\n",
    "                guess = all_letters[guess_idx]\n",
    "\n",
    "                while guess in guesses:\n",
    "                    arr[guess_idx] = -np.inf\n",
    "                    guess_idx = np.argmax(arr)\n",
    "                    guess = all_letters[guess_idx]\n",
    "\n",
    "            if guess in word_idxs:\n",
    "                for i in word_idxs[guess]:\n",
    "                    encoded_word = encoded_word[:i] + guess + encoded_word[i+1:]\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "            guesses[guess] = True\n",
    "\n",
    "            if verbose==1:\n",
    "                print(f\"  Guess: {guess.upper():<2} → {encoded_word}  (Wrong guesses: {num_wrong})\")\n",
    "\n",
    "        result = encoded_word == word\n",
    "        if verbose==2 or verbose==1:\n",
    "            print(f\"Result: {'✅ CORRECT' if result else '❌ FAILED'} | Final: {encoded_word} {guesses.keys()}\\n\")\n",
    "        return result\n",
    "    def test_accuracy(self, words, verbose=1):\n",
    "        n=len(words)\n",
    "        count=1\n",
    "        correct = 0\n",
    "        for w in words:\n",
    "            print(f\"{(correct / count) * 100:.2f} {count} / {n}\", end=\"\\r\")\n",
    "            correct += self.simulate_hangman_transformers(w, verbose=verbose)\n",
    "            count+=1\n",
    "        return correct / len(words)\n",
    "    \n",
    "# import torch\n",
    "# import numpy as np\n",
    "from transformers import CanineTokenizer, CanineConfig, CanineForSequenceClassification\n",
    "from transformers import CanineConfig, CanineTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# # Constants for separating and masking in the CANINE input sequence\n",
    "# CANINE_SEP_TOKEN   = \" [SEP] \"\n",
    "# CANINE_MASK_TOKEN  = \"[MASK]\"\n",
    "\n",
    "class CanineHangmanPlayer:\n",
    "    def __init__(self, pretrained_model_path: str = \"google/canine-s\", device: torch.device = None,lstm=None):\n",
    "        # Load CANINE tokenizer & config\n",
    "        self.tokenizer = CanineTokenizer.from_pretrained('google/canine-s')\n",
    "        self.config    = CanineConfig.from_pretrained(pretrained_model_path)\n",
    "        self.config.num_labels = 26\n",
    "        self.lstm_model=lstm\n",
    "        \n",
    "        # Set up device\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load a sequence-classification head on top of CANINE\n",
    "        self.model =AutoModelForSequenceClassification.from_pretrained(\n",
    "            pretrained_model_path,\n",
    "            config=self.config\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Tokens for building the game state string\n",
    "        self.CANINE_MASK_TOKEN = self.tokenizer.mask_token\n",
    "        self.CANINE_SEP_TOKEN = self.tokenizer.sep_token\n",
    "        \n",
    "        # Toggle for self-play finetuning\n",
    "        self.training = False\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def simulate_hangman_transformers(self, word: str, max_wrong_guesses: int = 6, verbose: int = 1):\n",
    "        \"\"\"\n",
    "        Play hangman against the CANINE model.\n",
    "        If self.training is True, returns (model_logits_seq, true_dist_seq, success_flag).\n",
    "        Otherwise returns just susccess_flag.\n",
    "        \"\"\"\n",
    "        # Build index map of letters→positions\n",
    "        word_idxs = {}\n",
    "        for i, c in enumerate(word):\n",
    "            word_idxs.setdefault(c, []).append(i)\n",
    "\n",
    "        all_letters = [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
    "        guesses      = {}\n",
    "        encoded_word = \"*\" * len(word)\n",
    "        num_wrong= 0\n",
    "\n",
    "        self.eval()\n",
    "        if self.training:\n",
    "            outputs_model = []\n",
    "            outputs_true  = []\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[WORD]: {word}\")\n",
    "\n",
    "        # Main guessing loop\n",
    "#         while encoded_word != word and wrong_count < max_wrong_guesses:\n",
    "#             # Build the input string\n",
    "#             state = (\n",
    "#                 ''.join(guesses.keys())\n",
    "#                 + self.CANINE_SEP_TOKEN\n",
    "#                 + encoded_word.replace(\"*\", self.CANINE_MASK_TOKEN)\n",
    "#             )\n",
    "#             enc = self.tokenizer(\n",
    "#                 state,\n",
    "#                 padding=\"max_length\",\n",
    "#                 truncation=True,\n",
    "#                 max_length=64,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             ).to(self.device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             with torch.no_grad():\n",
    "#                 out = self.model(**enc)\n",
    "#             logits = out.logits             # shape: (1, 26)\n",
    "#             arr_np = logits[0].cpu().numpy()\n",
    "\n",
    "#             # If training, build the “true” distribution for this step\n",
    "#             if self.training:\n",
    "#                 true_dist = torch.zeros(26, device=self.device)\n",
    "#                 for ch, positions in word_idxs.items():\n",
    "#                     if ch not in guesses:\n",
    "#                         true_dist[ord(ch) - ord('a')] = len(positions)\n",
    "#                 s = true_dist.sum().item()\n",
    "#                 if s > 0:\n",
    "#                     true_dist /= s\n",
    "#                 else:\n",
    "#                     raise ValueError(\"Invalid output distribution (sum=0).\")\n",
    "#                 outputs_model.append(logits)\n",
    "#                 outputs_true.append(true_dist)\n",
    "\n",
    "#             # Pick the highest-scoring unseen letter\n",
    "#             guess_idx = int(np.argmax(arr_np))\n",
    "#             guess     = all_letters[guess_idx]\n",
    "#             while guess in guesses:\n",
    "#                 arr_np[guess_idx] = -np.inf\n",
    "#                 guess_idx = int(np.argmax(arr_np))\n",
    "#                 guess     = all_letters[guess_idx]\n",
    "        while encoded_word != word and num_wrong < max_wrong_guesses:\n",
    "            missing_count = encoded_word.count(\"*\")\n",
    "\n",
    "            if missing_count <= 0:\n",
    "                masked = encoded_word.replace(\"*\", \"0\")\n",
    "                guess = self.lstm_model.predict(masked, guessed_letters=set(guesses.keys()))\n",
    "                print(guess,end=\" \")\n",
    "            else:\n",
    "                state = ''.join(guesses.keys()) + self.CANINE_SEP_TOKEN + encoded_word.replace('*', self.CANINE_MASK_TOKEN)\n",
    "                enc = self.tokenizer(state, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(**enc).logits\n",
    "\n",
    "                arr = logits.cpu().numpy()[0]\n",
    "                guess_idx = np.argmax(arr)\n",
    "                guess = all_letters[guess_idx]\n",
    "\n",
    "                while guess in guesses:\n",
    "                    arr[guess_idx] = -np.inf\n",
    "                    guess_idx = np.argmax(arr)\n",
    "                    guess = all_letters[guess_idx]\n",
    "\n",
    "            # Apply the guess\n",
    "            if guess in word_idxs:\n",
    "                for pos in word_idxs[guess]:\n",
    "                    encoded_word = encoded_word[:pos] + guess + encoded_word[pos+1:]\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "            guesses[guess] = True\n",
    "            if verbose == 1:\n",
    "                print(f\"  Guess: {guess.upper():<2} → {encoded_word}  (Wrong: {num_wrong})\")\n",
    "\n",
    "        success = (encoded_word == word)\n",
    "        if verbose == 3:\n",
    "            print(f\"Result: {'✅ CORRECT' if success else '❌ FAILED'} | Final: {encoded_word} {word} {guesses.keys()} \\n\")\n",
    "\n",
    "        if self.training:\n",
    "            return torch.vstack(outputs_model), torch.vstack(outputs_true), success\n",
    "        return success\n",
    "\n",
    "#     def simulate_hangman_transformers(self, word, max_wrong_guesses=6, verbose=1):\n",
    "#         word_idxs = {}\n",
    "#         all_letters = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "#         for i, c in enumerate(word):\n",
    "#             word_idxs.setdefault(c, []).append(i)\n",
    "\n",
    "#         guesses = {}\n",
    "#         encoded_word = \"*\" * len(word)\n",
    "#         num_wrong = 0\n",
    "#         self.eval()\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"[WORD]: {word}\")\n",
    "\n",
    "#         while encoded_word != word and num_wrong < max_wrong_guesses:\n",
    "#             missing_count = encoded_word.count(\"*\")\n",
    "#             guessed_letters = set(guesses.keys())\n",
    "\n",
    "#             guess = None\n",
    "\n",
    "#             if missing_count > 1:\n",
    "#                 # Use ByT5 only\n",
    "#                 state = ''.join(guessed_letters) + BYT5_SEP_TOKEN + encoded_word.replace('*', BYT5_MASK_TOKEN)\n",
    "#                 enc = self.tokenizer(state, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     logits = self.model(**enc).logits\n",
    "#                 probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "#                 idx = np.argmax(probs)\n",
    "#                 guess = all_letters[idx]\n",
    "#                 while guess in guesses:\n",
    "#                     probs[idx] = -np.inf\n",
    "#                     idx = np.argmax(probs)\n",
    "#                     guess = all_letters[idx]\n",
    "\n",
    "#             else:\n",
    "#                 # Use both models and compare probabilities\n",
    "#                 # --- LSTM ---\n",
    "#                 masked = encoded_word.replace(\"*\", \"0\")\n",
    "#                 word_encoded = [self.lstm_model.char_to_int.get(char, self.lstm_model.char_to_int['0']) for char in masked]\n",
    "#                 word_padded = pad_sequences([word_encoded], maxlen=self.lstm_model.max_word_length, padding='post')\n",
    "#                 lstm_pred = self.lstm_model.model.predict(word_padded, verbose=0)[0]\n",
    "\n",
    "#                 lstm_guess, lstm_prob = None, -1\n",
    "#                 for i, char in enumerate(masked):\n",
    "#                     if char == '0':\n",
    "#                         probs = lstm_pred[i]\n",
    "#                         for idx in np.argsort(-probs):\n",
    "#                             c = self.lstm_model.int_to_char[idx]\n",
    "#                             if c != '0' and c not in guessed_letters:\n",
    "#                                 lstm_guess = c\n",
    "#                                 lstm_prob = probs[idx]\n",
    "#                                 break\n",
    "#                         break\n",
    "\n",
    "#                 # --- ByT5 ---\n",
    "#                 state = ''.join(guessed_letters) + BYT5_SEP_TOKEN + encoded_word.replace('*', BYT5_MASK_TOKEN)\n",
    "#                 enc = self.tokenizer(state, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     logits = self.model(**enc).logits\n",
    "#                 probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "#                 byt5_guess, byt5_prob = None, -1\n",
    "#                 idx = np.argmax(probs)\n",
    "#                 byt5_guess = all_letters[idx]\n",
    "#                 byt5_prob = probs[idx]\n",
    "#                 while byt5_guess in guesses:\n",
    "#                     probs[idx] = -np.inf\n",
    "#                     idx = np.argmax(probs)\n",
    "#                     byt5_guess = all_letters[idx]\n",
    "#                     byt5_prob = probs[idx]\n",
    "\n",
    "#                 # Compare both\n",
    "#                 guess = lstm_guess if lstm_prob > byt5_prob else byt5_guess\n",
    "\n",
    "#             if guess in word_idxs:\n",
    "#                 for i in word_idxs[guess]:\n",
    "#                     encoded_word = encoded_word[:i] + guess + encoded_word[i+1:]\n",
    "#             else:\n",
    "#                 num_wrong += 1\n",
    "\n",
    "#             guesses[guess] = True\n",
    "\n",
    "#             if verbose == 1:\n",
    "#                 print(f\"  Guess: {guess.upper():<2} → {encoded_word}  (Wrong guesses: {num_wrong})\")\n",
    "\n",
    "#         result = encoded_word == word\n",
    "#         if verbose == 2:\n",
    "#             print(f\"Result: {'✅ CORRECT' if result else '❌ FAILED'} | Final: {encoded_word}\\n\")\n",
    "#         return result\n",
    "    def test_accuracy(self, words, verbose=1):\n",
    "        n=len(words)\n",
    "        count=1\n",
    "        correct = 0\n",
    "        for w in words:\n",
    "            print(f\"{(correct / count) * 100:.2f} {count} / {n}\", end=\"\\r\")\n",
    "            correct += self.simulate_hangman_transformers(w, verbose=verbose)\n",
    "            count+=1\n",
    "        return correct / len(words)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # val_words = [\"melon\", \"berry\", \"lemon\", \"plum\", \"kiwi\"]\n",
    "#     player = ByT5HangmanPlayer()\n",
    "#     count=0\n",
    "#     acc = player.test_accuracy(val_words, verbose=0)\n",
    "#     print(f\"\\n📊 Final Gameplay Validation Accuracy: {acc:.4f}\")\n",
    "# if __name__ == \"__main__\":\n",
    "#     # A small set of words to validate on\n",
    "#     # val_words = [\"melon\", \"berry\", \"lemon\", \"plum\", \"kiwi\"]\n",
    "    \n",
    "#     # Initialize your CANINE-based player\n",
    "#     player = CanineHangmanPlayer(pretrained_model_path=\"byt5-checkpoint-epoch5c\")\n",
    "    \n",
    "#     # (Optional) enable self-play logging\n",
    "#     player.training = False\n",
    "#     n=len(val_words)\n",
    "#     # Run through all validation words\n",
    "#     correct = 0\n",
    "#     count=1\n",
    "#     for word in val_words:\n",
    "#         print(f\"{(correct / count) * 100:.2f} {count} / {n}\", end=\"\\r\")\n",
    "#         win = player.simulate_hangman_transformers(word, max_wrong_guesses=6, verbose=1)\n",
    "#         correct += int(win)\n",
    "#         count+=1\n",
    "    \n",
    "#     # Compute and print accuracy\n",
    "#     accuracy = correct / len(val_words)\n",
    "#     print(f\"\\n📊 Final Gameplay Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb448c49-bae6-42a1-897e-068f52170e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-15 03:04:08.099445: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.105401: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.105567: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.106826: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.106947: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.107026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.209299: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.209432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.209525: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-05-15 03:04:08.209603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46866 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# from CanineHangmanPlayer import CanineHangmanPlayer\n",
    "lstm=LSTMWordPredictor()\n",
    "model=CanineHangmanPlayer(\"canine-pretrained-hangman-log/6c\",lstm=lstm)\n",
    "model.training=False\n",
    "\n",
    "# player = ByT5HangmanPlayer(lstm=lstm)\n",
    "# player.simulate_hangman_transformers(w,verbose=2)\n",
    "# print(model.test_accuracy(val_words))\n",
    "# simulate_hangman_transformers(self, word, max_wrong_guesses=6, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc9b80-1bb4-40d7-9545-400380018add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n=len(val_words)\n",
    "count=1\n",
    "correct1 = 0\n",
    "correct2 = 0\n",
    "for w in val_words:\n",
    "    print(f\"{(correct1 / count) * 100:.2f} {(correct1 / count) * 100:.2f}  {count} / {n}\", end=\"\\r\")\n",
    "    correct1 += model.simulate_hangman_transformers(w, verbose=3)\n",
    "    # correct2 += player.simulate_hangman_transformers(w,verbose=0)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92bb445-1e88-4884-abd1-964b05e95fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    ByT5Tokenizer,\n",
    "    T5Config,\n",
    "    T5ForSequenceClassification,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "BYT5_MASK_TOKEN = \"<extra_id_0>\"\n",
    "BYT5_SEP_TOKEN = \"<sep>\"\n",
    "\n",
    "class ByT5HangmanPlayer:\n",
    "    def __init__(self, saved_model_path=None):\n",
    "        self.tokenizer = ByT5Tokenizer.from_pretrained(\"google/byt5-small\")\n",
    "        self.config = T5Config.from_pretrained(\"google/byt5-small\")\n",
    "        self.config.num_labels = 26\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # if saved_model_path:\n",
    "        #     self.model = T5ForSequenceClassification.from_pretrained(saved_model_path, config=self.config).to(self.device)\n",
    "        # else:\n",
    "        #     self.model = T5ForSequenceClassification.from_pretrained(\"google/byt5-small\", config=self.config).to(self.device)\n",
    "        self.model = T5ForSequenceClassification.from_pretrained(\"byt5-checkpoint-epoch6b\", config=self.config).to(self.device)\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "\n",
    "    def simulate_hangman_transformers(self, word, max_wrong_guesses=6, verbose=1):\n",
    "        word_idxs = {}\n",
    "        all_letters = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "        for i, c in enumerate(word):\n",
    "            word_idxs.setdefault(c, []).append(i)\n",
    "\n",
    "        guesses = {}\n",
    "        encoded_word = \"*\" * len(word)\n",
    "        num_wrong = 0\n",
    "        self.eval()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n[WORD]: {word}\")\n",
    "\n",
    "        while encoded_word != word and num_wrong < max_wrong_guesses:\n",
    "            state = ''.join(guesses.keys()) + BYT5_SEP_TOKEN + encoded_word.replace('*', BYT5_MASK_TOKEN)\n",
    "            enc = self.tokenizer(state, padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = self.model(**enc).logits\n",
    "\n",
    "            arr = logits.cpu().numpy()[0]\n",
    "            guess_idx = np.argmax(arr)\n",
    "            guess = all_letters[guess_idx]\n",
    "\n",
    "            while guess in guesses:\n",
    "                arr[guess_idx] = -np.inf\n",
    "                guess_idx = np.argmax(arr)\n",
    "                guess = all_letters[guess_idx]\n",
    "\n",
    "            if guess in word_idxs:\n",
    "                for i in word_idxs[guess]:\n",
    "                    encoded_word = encoded_word[:i] + guess + encoded_word[i+1:]\n",
    "            else:\n",
    "                num_wrong += 1\n",
    "\n",
    "            guesses[guess] = True\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"  Guess: {guess.upper():<2} → {encoded_word}  (Wrong guesses: {num_wrong})\")\n",
    "\n",
    "        result = encoded_word == word\n",
    "        if verbose:\n",
    "            print(f\"Result: {'✅ CORRECT' if result else '❌ FAILED'} | Final: {encoded_word}\\n\")\n",
    "        return result\n",
    "\n",
    "    def test_accuracy(self, words, verbose=1):\n",
    "        correct = 0\n",
    "        for w in words:\n",
    "            correct += self.simulate_hangman_transformers(w, verbose=verbose)\n",
    "        return correct / len(words)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    import sys\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_path\", type=str, default=None, help=\"Path to pretrained ByT5 model\")\n",
    "    args = parser.parse_args(args=[] if sys.argv[0].endswith(\"ipykernel_launcher.py\") else None)\n",
    "\n",
    "    # Sample validation words\n",
    "    # val_words = [\"melon\", \"berry\", \"lemon\", \"plum\", \"kiwi\"]\n",
    "\n",
    "    # Initialize model\n",
    "    player = ByT5HangmanPlayer(saved_model_path=args.model_path)\n",
    "\n",
    "    # Run validation with detailed logs\n",
    "    acc = player.test_accuracy(val_words, verbose=1)\n",
    "    print(f\"\\n📊 Final Gameplay Validation Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8934821d-5572-4d13-9492-18e541859896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
